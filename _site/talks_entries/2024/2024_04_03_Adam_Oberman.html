<!DOCTYPE html>




<html
 dir="ltr"
 lang="en"
 class="has-navbar-fixed-top">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content=#ffffff>
    <link rel="stylesheet" href="/assets/css/app.css">
    <link rel="shortcut icon" type="image/png"
           href="/favicon.png" 
    />
    <script defer src="https://unpkg.com/alpinejs@3.9.0/dist/cdn.min.js"></script>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css">
    
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma-social@1/bin/bulma-social.min.css">
    
    <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Theoretical insights into self-supervised feature representation learning. | Montreal MLOpt</title>
<meta name="generator" content="Jekyll v3.9.3" />
<meta property="og:title" content="Theoretical insights into self-supervised feature representation learning." />
<meta name="author" content="Adam Oberman" />
<meta property="og:locale" content="en" />
<meta name="description" content="Adam Oberman is a McGill mathematics Professor and CIFAR AI chair on sabbatical at Mila this year. He teaches two Machine Learning classes, including one on generalization and stability theory. In this theory-oriented talk aimed at a general audience, I will present recent work by a number of authors who have developed a theory for how deep neural network learn features from data augmentation. Despite using data without labels, these features are effective for downstream classification tasks: in fact, they achieve the same accuracy as supervised models, using only a fraction of the labels. My interest is in understanding if how this phenomena can give insight into feature representation learning in general: how do we train a network to learn features useful for downstream tasks? I’ll present some background material on generalization theory, and the stability approach to generalization, which is very compatible with convex optimization. Then I’ll present recent work that shows that many SLL algorithms correspond to minimizing the same loss for features. This loss is a generalization of a functional PCA type loss. Using this loss along with the appropriate stability/generalization theory, we can then show that linear classifiers using these can learn classifiers." />
<meta property="og:description" content="Adam Oberman is a McGill mathematics Professor and CIFAR AI chair on sabbatical at Mila this year. He teaches two Machine Learning classes, including one on generalization and stability theory. In this theory-oriented talk aimed at a general audience, I will present recent work by a number of authors who have developed a theory for how deep neural network learn features from data augmentation. Despite using data without labels, these features are effective for downstream classification tasks: in fact, they achieve the same accuracy as supervised models, using only a fraction of the labels. My interest is in understanding if how this phenomena can give insight into feature representation learning in general: how do we train a network to learn features useful for downstream tasks? I’ll present some background material on generalization theory, and the stability approach to generalization, which is very compatible with convex optimization. Then I’ll present recent work that shows that many SLL algorithms correspond to minimizing the same loss for features. This loss is a generalization of a functional PCA type loss. Using this loss along with the appropriate stability/generalization theory, we can then show that linear classifiers using these can learn classifiers." />
<link rel="canonical" href="http://localhost:4000/talks_entries/2024/2024_04_03_Adam_Oberman.html" />
<meta property="og:url" content="http://localhost:4000/talks_entries/2024/2024_04_03_Adam_Oberman.html" />
<meta property="og:site_name" content="Montreal MLOpt" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-04-03T13:00:00-04:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Theoretical insights into self-supervised feature representation learning." />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Adam Oberman"},"dateModified":"2024-04-03T13:00:00-04:00","datePublished":"2024-04-03T13:00:00-04:00","description":"Adam Oberman is a McGill mathematics Professor and CIFAR AI chair on sabbatical at Mila this year. He teaches two Machine Learning classes, including one on generalization and stability theory. In this theory-oriented talk aimed at a general audience, I will present recent work by a number of authors who have developed a theory for how deep neural network learn features from data augmentation. Despite using data without labels, these features are effective for downstream classification tasks: in fact, they achieve the same accuracy as supervised models, using only a fraction of the labels. My interest is in understanding if how this phenomena can give insight into feature representation learning in general: how do we train a network to learn features useful for downstream tasks? I’ll present some background material on generalization theory, and the stability approach to generalization, which is very compatible with convex optimization. Then I’ll present recent work that shows that many SLL algorithms correspond to minimizing the same loss for features. This loss is a generalization of a functional PCA type loss. Using this loss along with the appropriate stability/generalization theory, we can then show that linear classifiers using these can learn classifiers.","headline":"Theoretical insights into self-supervised feature representation learning.","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/talks_entries/2024/2024_04_03_Adam_Oberman.html"},"url":"http://localhost:4000/talks_entries/2024/2024_04_03_Adam_Oberman.html"}</script>
<!-- End Jekyll SEO tag -->

    <!-- head scripts --></head>

  <body>
    <nav class="navbar is-primary  is-fixed-top " x-data="{ openNav: false }">
    <div class="container">
        <div class="navbar-brand">
            <a href="/" class="navbar-item">
                Montreal MLOpt
            </a>
            <a role="button" class="navbar-burger burger" aria-label="menu" aria-expanded="false" data-target="navMenu" :class="{ 'is-active': openNav }" x-on:click="openNav = !openNav">
                <span aria-hidden="true"></span>
                <span aria-hidden="true"></span>
                <span aria-hidden="true"></span>
            </a>
        </div>
        <div class="navbar-menu" id="navMenu" :class="{ 'is-active': openNav }">
            <div class="navbar-start">
                <a href="/" class="navbar-item ">Home</a>
                
                
                    
                    <a href="/talks" class="navbar-item ">Talks schedule</a>
                    
                
                    
                    <div class="navbar-item has-dropdown is-hoverable ">
                        <a href="/organizers" class="navbar-link ">Organizers</a>
                        <div class="navbar-dropdown">
                            
                            <a href="/organizers" class="navbar-item ">All</a>
                            
                            <a href="/organizers#current" class="navbar-item ">Current</a>
                            
                            <a href="/organizers#alumnis" class="navbar-item ">Past organizers</a>
                            
                        </div>
                    </div>
                    
                
                    
                    <a href="/archive/index.html" class="navbar-item ">Archive</a>
                    
                
                
            </div>

            <div class="navbar-end">
                
            </div>

        </div>
    </div>
</nav>

    
        <section class="hero  is-small  is-bold is-primary" >
    <div class="hero-body ">
        <div class="container">
            <h1 class="title is-2">Theoretical insights into self-supervised feature representation learning.</h1>
            <p class="subtitle is-3"></p>
            
        </div>
    </div>
</section>
    
    


    <section class="section">
        <div class="container">
            <div class="columns is-multiline">
                
                <div class="column is-12">
                    
                    

                    
                    
                    
<div class="content">
    <h2> Speaker: Adam Oberman </h2>

<div> <b>Where:</b> Microsoft Research Labs.</div> 
<div> <b>When:</b> April 03, 2024 at 13:00.</div>








<h2> Abstract </h2>

<div> <p>Adam Oberman is a McGill mathematics Professor and CIFAR AI chair on sabbatical at Mila this year. He teaches two Machine Learning classes, including one on generalization and stability theory.
In this theory-oriented talk aimed at a general audience, I will present recent work by a number of authors who have developed a theory for how deep neural network learn features from data augmentation. Despite using data without labels, these features are effective for downstream classification tasks: in fact, they achieve the same accuracy as supervised models, using only a fraction of the labels.
My interest is in understanding if how this phenomena can give insight into feature representation learning in general: how do we train a network to learn features useful for downstream tasks?
I’ll present some background material on generalization theory, and the stability approach to generalization, which is very compatible with convex optimization.
Then I’ll present recent work that shows that many SLL algorithms correspond to minimizing the same loss for features. This loss is a generalization of a functional PCA type loss.
Using this loss along with the appropriate stability/generalization theory, we can then show that linear classifiers using these can learn classifiers.</p>
</div>


<div class="tags">
    
</div>


<p><strong>Share</strong></p>
<div class="buttons ">
    <a class="button is-medium is-facebook"
       onclick="window.open('https://www.facebook.com/share.php?u=http://localhost:4000/talks_entries/2024/2024_04_03_Adam_Oberman.html');">
        <span class="icon"><i class="fab fa-facebook fa-lg"></i></span>
    </a>
    <a class="button is-medium is-twitter"
       onclick="window.open('https://twitter.com/intent/tweet?text=http://localhost:4000/talks_entries/2024/2024_04_03_Adam_Oberman.html');">
        <span class="icon"><i class="fab fa-twitter fa-lg"></i></span>
    </a>
    <a class="button is-medium is-linkedin"
       onclick="window.open('https://www.linkedin.com/shareArticle?mini=true&url=http://localhost:4000/talks_entries/2024/2024_04_03_Adam_Oberman.html&title=Theoretical+insights+into+self-supervised+feature+representation+learning.&summary=&source=');">
        <span class="icon"><i class="fab fa-linkedin fa-lg"></i></span>
    </a>
    <a class="button is-medium is-reddit"
       onclick="window.open('https://reddit.com/submit?url=http://localhost:4000/talks_entries/2024/2024_04_03_Adam_Oberman.html');">
        <span class="icon"><i class="fab fa-reddit fa-lg"></i></span>
    </a>
</div>



</div>
                </div>
                
            </div>
        </div>
    </section>
    
        <footer class="footer">
    <div class="container">
        
        

        <div class="content is-small has-text-centered">
            <p class="">Theme built by <a href="https://www.csrhymes.com">C.S. Rhymes</a></p>
        </div>
    </div>
</footer>

    
    <script src="/assets/js/app.js" type="text/javascript"></script><!-- footer scripts --></body>
</html>
