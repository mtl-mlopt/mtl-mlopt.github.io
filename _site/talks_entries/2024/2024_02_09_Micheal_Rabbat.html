<!DOCTYPE html>




<html
 dir="ltr"
 lang="en"
 class="has-navbar-fixed-top">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content=#ffffff>
    <link rel="stylesheet" href="/assets/css/app.css">
    <link rel="shortcut icon" type="image/png"
           href="/favicon.png" 
    />
    <script defer src="https://unpkg.com/alpinejs@3.9.0/dist/cdn.min.js"></script>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css">
    
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma-social@1/bin/bulma-social.min.css">
    
    <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Benchmarking Neural Network Training Algorithms | Montreal MLOpt</title>
<meta name="generator" content="Jekyll v3.9.3" />
<meta property="og:title" content="Benchmarking Neural Network Training Algorithms" />
<meta name="author" content="Michael Rabbat" />
<meta property="og:locale" content="en" />
<meta name="description" content="Training algorithms are an essential part of every deep learning pipeline. Training algorithm improvements that speed up training across a wide variety of workloads (e.g., better update rules, tuning protocols, learning rate schedules, or data selection schemes) could save time, save computational resources, and lead to better, more accurate, models. Unfortunately, as a community, we are currently unable to reliably identify training algorithm improvements, or even determine the state-of-the-art training algorithm. In this work, using concrete experiments, we argue that real progress in speeding up training requires new benchmarks that resolve three basic challenges faced by empirical comparisons of training algorithms: (1) how to decide when training is complete and precisely measure training time, (2) how to handle the sensitivity of measurements to exact workload details, and (3) how to fairly compare algorithms that require hyperparameter tuning. To address these challenges, we introduce a new, competitive, time-to-result benchmark using multiple workloads running on fixed hardware, the AlgoPerf: Training Algorithms benchmark. Our benchmark includes a set of workload variants that make it possible to detect benchmark submissions that are more robust to workload changes than current widely-used methods. Finally, we evaluate baseline submissions constructed using various optimizers that represent current practice, as well as other optimizers that have recently received attention in the literature. These baseline results collectively demonstrate the feasibility of our benchmark, show that non-trivial gaps between methods exist, and set a provisional state-of-the-art for future benchmark submissions to try and surpass. This is joint work with the MLCommons Algorithmic Efficiency Working Group." />
<meta property="og:description" content="Training algorithms are an essential part of every deep learning pipeline. Training algorithm improvements that speed up training across a wide variety of workloads (e.g., better update rules, tuning protocols, learning rate schedules, or data selection schemes) could save time, save computational resources, and lead to better, more accurate, models. Unfortunately, as a community, we are currently unable to reliably identify training algorithm improvements, or even determine the state-of-the-art training algorithm. In this work, using concrete experiments, we argue that real progress in speeding up training requires new benchmarks that resolve three basic challenges faced by empirical comparisons of training algorithms: (1) how to decide when training is complete and precisely measure training time, (2) how to handle the sensitivity of measurements to exact workload details, and (3) how to fairly compare algorithms that require hyperparameter tuning. To address these challenges, we introduce a new, competitive, time-to-result benchmark using multiple workloads running on fixed hardware, the AlgoPerf: Training Algorithms benchmark. Our benchmark includes a set of workload variants that make it possible to detect benchmark submissions that are more robust to workload changes than current widely-used methods. Finally, we evaluate baseline submissions constructed using various optimizers that represent current practice, as well as other optimizers that have recently received attention in the literature. These baseline results collectively demonstrate the feasibility of our benchmark, show that non-trivial gaps between methods exist, and set a provisional state-of-the-art for future benchmark submissions to try and surpass. This is joint work with the MLCommons Algorithmic Efficiency Working Group." />
<link rel="canonical" href="http://localhost:4000/talks_entries/2024/2024_02_09_Micheal_Rabbat.html" />
<meta property="og:url" content="http://localhost:4000/talks_entries/2024/2024_02_09_Micheal_Rabbat.html" />
<meta property="og:site_name" content="Montreal MLOpt" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-02-09T13:00:00-05:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Benchmarking Neural Network Training Algorithms" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Michael Rabbat"},"dateModified":"2024-02-09T13:00:00-05:00","datePublished":"2024-02-09T13:00:00-05:00","description":"Training algorithms are an essential part of every deep learning pipeline. Training algorithm improvements that speed up training across a wide variety of workloads (e.g., better update rules, tuning protocols, learning rate schedules, or data selection schemes) could save time, save computational resources, and lead to better, more accurate, models. Unfortunately, as a community, we are currently unable to reliably identify training algorithm improvements, or even determine the state-of-the-art training algorithm. In this work, using concrete experiments, we argue that real progress in speeding up training requires new benchmarks that resolve three basic challenges faced by empirical comparisons of training algorithms: (1) how to decide when training is complete and precisely measure training time, (2) how to handle the sensitivity of measurements to exact workload details, and (3) how to fairly compare algorithms that require hyperparameter tuning. To address these challenges, we introduce a new, competitive, time-to-result benchmark using multiple workloads running on fixed hardware, the AlgoPerf: Training Algorithms benchmark. Our benchmark includes a set of workload variants that make it possible to detect benchmark submissions that are more robust to workload changes than current widely-used methods. Finally, we evaluate baseline submissions constructed using various optimizers that represent current practice, as well as other optimizers that have recently received attention in the literature. These baseline results collectively demonstrate the feasibility of our benchmark, show that non-trivial gaps between methods exist, and set a provisional state-of-the-art for future benchmark submissions to try and surpass. This is joint work with the MLCommons Algorithmic Efficiency Working Group.","headline":"Benchmarking Neural Network Training Algorithms","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/talks_entries/2024/2024_02_09_Micheal_Rabbat.html"},"url":"http://localhost:4000/talks_entries/2024/2024_02_09_Micheal_Rabbat.html"}</script>
<!-- End Jekyll SEO tag -->

    <!-- head scripts --></head>

  <body>
    <nav class="navbar is-primary  is-fixed-top " x-data="{ openNav: false }">
    <div class="container">
        <div class="navbar-brand">
            <a href="/" class="navbar-item">
                Montreal MLOpt
            </a>
            <a role="button" class="navbar-burger burger" aria-label="menu" aria-expanded="false" data-target="navMenu" :class="{ 'is-active': openNav }" x-on:click="openNav = !openNav">
                <span aria-hidden="true"></span>
                <span aria-hidden="true"></span>
                <span aria-hidden="true"></span>
            </a>
        </div>
        <div class="navbar-menu" id="navMenu" :class="{ 'is-active': openNav }">
            <div class="navbar-start">
                <a href="/" class="navbar-item ">Home</a>
                
                
                    
                    <a href="/talks" class="navbar-item ">Talks schedule</a>
                    
                
                    
                    <div class="navbar-item has-dropdown is-hoverable ">
                        <a href="/organizers" class="navbar-link ">Organizers</a>
                        <div class="navbar-dropdown">
                            
                            <a href="/organizers" class="navbar-item ">All</a>
                            
                            <a href="/organizers#current" class="navbar-item ">Current</a>
                            
                            <a href="/organizers#alumnis" class="navbar-item ">Past organizers</a>
                            
                        </div>
                    </div>
                    
                
                    
                    <a href="/archive/index.html" class="navbar-item ">Archive</a>
                    
                
                
            </div>

            <div class="navbar-end">
                
            </div>

        </div>
    </div>
</nav>

    
        <section class="hero  is-small  is-bold is-primary" >
    <div class="hero-body ">
        <div class="container">
            <h1 class="title is-2">Benchmarking Neural Network Training Algorithms</h1>
            <p class="subtitle is-3"></p>
            
        </div>
    </div>
</section>
    
    


    <section class="section">
        <div class="container">
            <div class="columns is-multiline">
                
                <div class="column is-12">
                    
                    

                    
                    
                    
<div class="content">
    <h2> Speaker: Michael Rabbat </h2>

<div> <b>Where:</b> Microsoft Research Labs.</div> 
<div> <b>When:</b> February 09, 2024 at 13:00.</div>




<h3> Reference(s) </h3>
<ul>

<li>  <a href="https://arxiv.org/abs/2306.07179"> https://arxiv.org/abs/2306.07179 </a> </li>

</ul>



<h3> Miscellaneous </h3>
<ul>
  
    <li>Github: <a href="https://github.com/mlcommons/algorithmic-efficiency/">https://github.com/mlcommons/algorithmic-efficiency/</a></li>
  
    <li>Call for submissions: <a href="https://github.com/mlcommons/algorithmic-efficiency/blob/main/CALL_FOR_SUBMISSIONS.md">https://github.com/mlcommons/algorithmic-efficiency/blob/main/CALL_FOR_SUBMISSIONS.md</a></li>
  
</ul>



<h2> Abstract </h2>

<div> <p>Training algorithms are an essential part of every deep learning pipeline. Training algorithm improvements that speed up training across a wide variety of workloads (e.g., better update rules, tuning protocols, learning rate schedules, or data selection schemes) could save time, save computational resources, and lead to better, more accurate, models. Unfortunately, as a community, we are currently unable to reliably identify training algorithm improvements, or even determine the state-of-the-art training algorithm. In this work, using concrete experiments, we argue that real progress in speeding up training requires new benchmarks that resolve three basic challenges faced by empirical comparisons of training algorithms: (1) how to decide when training is complete and precisely measure training time, (2) how to handle the sensitivity of measurements to exact workload details, and (3) how to fairly compare algorithms that require hyperparameter tuning. To address these challenges, we introduce a new, competitive, time-to-result benchmark using multiple workloads running on fixed hardware, the AlgoPerf: Training Algorithms benchmark. Our benchmark includes a set of workload variants that make it possible to detect benchmark submissions that are more robust to workload changes than current widely-used methods. Finally, we evaluate baseline submissions constructed using various optimizers that represent current practice, as well as other optimizers that have recently received attention in the literature. These baseline results collectively demonstrate the feasibility of our benchmark, show that non-trivial gaps between methods exist, and set a provisional state-of-the-art for future benchmark submissions to try and surpass. This is joint work with the MLCommons Algorithmic Efficiency Working Group.</p>
</div>


<div class="tags">
    
</div>


<p><strong>Share</strong></p>
<div class="buttons ">
    <a class="button is-medium is-facebook"
       onclick="window.open('https://www.facebook.com/share.php?u=http://localhost:4000/talks_entries/2024/2024_02_09_Micheal_Rabbat.html');">
        <span class="icon"><i class="fab fa-facebook fa-lg"></i></span>
    </a>
    <a class="button is-medium is-twitter"
       onclick="window.open('https://twitter.com/intent/tweet?text=http://localhost:4000/talks_entries/2024/2024_02_09_Micheal_Rabbat.html');">
        <span class="icon"><i class="fab fa-twitter fa-lg"></i></span>
    </a>
    <a class="button is-medium is-linkedin"
       onclick="window.open('https://www.linkedin.com/shareArticle?mini=true&url=http://localhost:4000/talks_entries/2024/2024_02_09_Micheal_Rabbat.html&title=Benchmarking+Neural+Network+Training+Algorithms&summary=&source=');">
        <span class="icon"><i class="fab fa-linkedin fa-lg"></i></span>
    </a>
    <a class="button is-medium is-reddit"
       onclick="window.open('https://reddit.com/submit?url=http://localhost:4000/talks_entries/2024/2024_02_09_Micheal_Rabbat.html');">
        <span class="icon"><i class="fab fa-reddit fa-lg"></i></span>
    </a>
</div>



</div>
                </div>
                
            </div>
        </div>
    </section>
    
        <footer class="footer">
    <div class="container">
        
        

        <div class="content is-small has-text-centered">
            <p class="">Theme built by <a href="https://www.csrhymes.com">C.S. Rhymes</a></p>
        </div>
    </div>
</footer>

    
    <script src="/assets/js/app.js" type="text/javascript"></script><!-- footer scripts --></body>
</html>
